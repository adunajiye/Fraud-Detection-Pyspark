2. stream processes using kafka streams

from kafka import KafkaConsumer, KafkaProducer
from google.cloud import storage
from google.cloud import pubsub_v1
import json
from datetime import datetime
from threading import Thread
import os

# Google Cloud Pub/Sub parameters
project_id = 'your_project_id'
pubsub_subscription = 'your_pubsub_subscription_name'

# Kafka parameters
kafka_bootstrap_servers = 'your_kafka_broker_address:9092'
kafka_input_topic = 'input_topic'
kafka_output_topic = 'output_topic'

# Google Cloud Storage parameters
gcs_bucket_name = 'your_gcs_bucket_name'
gcs_folder_name = 'processed_data'

# Function to consume messages from Pub/Sub and produce them to Kafka input topic
def pubsub_to_kafka():
    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(project_id, pubsub_subscription)
    consumer = KafkaProducer(bootstrap_servers=kafka_bootstrap_servers)
    
    def callback(message):
        # Publish the message to Kafka input topic
        consumer.send(kafka_input_topic, value=message.data)
        message.ack()
    
    subscriber.subscribe(subscription_path, callback=callback)
    print('Subscribed to Pub/Sub topic and publishing to Kafka input topic...')

# Function to process messages using Kafka Streams
def process_messages():
    consumer = KafkaConsumer(kafka_input_topic, bootstrap_servers=kafka_bootstrap_servers)
    storage_client = storage.Client()
    bucket = storage_client.bucket(gcs_bucket_name)
    
    for message in consumer:
        # Process the message (e.g., perform transformations)
        processed_message = message.value.upper()  # Example: Convert message to uppercase
        
        # Upload the processed message to GCS
        file_name = f'{gcs_folder_name}/processed_data_{datetime.now().strftime("%Y-%m-%d_%H-%M-%S")}.json'
        blob = bucket.blob(file_name)
        blob.upload_from_string(json.dumps({'data': processed_message}), content_type='application/json')
        print(f'Processed message uploaded to GCS: {file_name}')

# Main function to run Pub/Sub to Kafka and Kafka Streams processing
def main():
    # Start Pub/Sub to Kafka data transfer
    pubsub_to_kafka_thread = Thread(target=pubsub_to_kafka)
    pubsub_to_kafka_thread.start()
    
    # Start Kafka Streams processing
    process_messages()

if __name__ == '__main__':
    main()


from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
from airflow.utils.email import send_email

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2022, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def run_python_script():
    os.system('python /path/to/your/script.py')

def send_email_alert(context):
    email_recipients = ['your@email.com']
    email_subject = 'Airflow DAG Alert'
    email_body = 'An error occurred in the data processing workflow.'
    
    execution_date = context['execution_date']
    execution_date_str = execution_date.strftime('%Y-%m-%d %H:%M:%S')
    email_subject_with_date = f'{email_subject} - {execution_date_str}'
    send_email(email_recipients, email_subject_with_date, email_body)

dag = DAG(
    'process_data_workflow',
    default_args=default_args,
    description='Orchestrate data processing workflow',
    schedule_interval=timedelta(minutes=5),
)

run_script_task = PythonOperator(
    task_id='run_script',
    python_callable=run_python_script,
    dag=dag,
)

send_email_task = PythonOperator(
    task_id='send_email',
    python_callable=send_email_alert,
    provide_context=True,
    dag=dag,
)

run_script_task >> send_email_task








option2 using dataflow 

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# Define the Pub/Sub topic and subscription
pubsub_topic = 'projects/{}/topics/{}'.format('your_project_id', 'your_pubsub_topic')
subscription = 'projects/{}/subscriptions/{}'.format('your_project_id', 'your_pubsub_subscription')

# Define the GCS output path
output_path = 'gs://your_bucket_name/output'

# Apache Beam pipeline options
options = PipelineOptions()

def process_pubsub_message(message):
    # Parse the message
    data = message.data.decode('utf-8')
    yield data

def run_pipeline():
    with beam.Pipeline(options=options) as pipeline:
        # Read messages from Pub/Sub
        messages = (
            pipeline
            | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=subscription)
            | 'Process Pub/Sub message' >> beam.FlatMap(process_pubsub_message)
        )

        # Write data to GCS
        messages | 'Write to GCS' >> beam.io.WriteToText(output_path)

if __name__ == "__main__":
    run_pipeline()


def process_pubsub_message(message):
    # Parse the message
    data = message.data.decode('utf-8')
    
    # Print the message if it exists along with any metadata
    if data:
        print("Received message ID:", message.message_id)
        print("Publish time:", message.publish_time)
        print("Message data:", data)
    
    yield data


